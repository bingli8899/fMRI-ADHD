notes: | 
  One layer without residual and normalization
  Without global attention pool 
  metadata has go through one MLP
  Two layer mlps after concatenation
  0.5 dropout with residuals

### Model Settings
model_name: "SageGNN_model" # Choose from: GCN, GATv2, DirGNN, SAGE-GNN
device: "cpu"

model_params:
  in_channels: 200
  hidden_channels: 32
  out_channels: 32
  norm: "batch_norm"
  pooling_function: "global_mean_pool" 
  # pooling function chosen from: global_mean_pool, global_add_pool, global_sort_pool, global_max_pool 
  loss_weights: [0.5, 0.5]
  label_smoothing: 0.05
  #DirGNNConv specific paras: 
  alpha: 0.5
  # GCN model specific: 
  undirectional_graph: False
  # GATv2model specific: 
  heads: 4
  concate: True
  drop_out_GAT: 0.5
  use_attention_pool: False
  batch_norm: False
  residual: True
  # SAGEmodel specific: 
  aggr: "max" 
  normalize: False 
  project: False

node_features: 
  identity: False
  correlation_matrix: True
  masking: True
  mask_prob: 0.01

predictor_paras: 
  norm_enabled: False # If add a normalization layer before predictor and before adding to metadata 
  norm_type: "batchnorm"  # or "layernorm"
  norm_timing: "before_meta" # or after 
  # If norm_timing is "before", then a normalization layer is appied before concating with metadata -> normalize only GCN output
  # If norm_time is "after", then a normalization layer is applied after concating with metadata -> normlize everything 
  predictor_type: Linear 
  num_pred_layers: 2
  activation_btw_predictors: ReLU
  hidden_channels: 64

### Data
root_folder: "/u/b/i/bingl/private/fMRI-AHDH/data"
# root_folder: "C:\\Users\\tosso\\Documents\\widsdatathon2025"
add_metadata: True
metadata_MLP: False
metada_MLP_out_channels: 64
resampling_enabled: True

### Classification type 
task: sex # adhd, four

### Optimizer

### Hyperparameters
num_folds: 5 # Cross validation
lr: 0.0001
num_layers: 1
dropout: 0.3
batch_size: 16
master_seed: 3407
patience: 20
num_epochs: 250

### scaler: 
scaling:
  enabled: True
  scaler: "MeanStd"

### output
checkpoint_dir: './checkpoints'
wandb:
  enabled: True
  project: ADHD-fMRI

# This below config is useless:
# The code works but not that good. 
run_inference_on_all_trainning: False # If true, the model will be evaluated on the whole trainning set to make inference
stop_num_epochs: 70 # 60 -80 is typically good for running whole training set 