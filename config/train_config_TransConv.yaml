notes: | 
  Transformer 

### Model Settings
model_name: "TransConv_model"
device: "cpu"

run_inference_on_all_trainning: False # If true, the model will be evaluated on the whole trainning set to make inference
stop_num_epochs: 70 # 60 -80 is typically good for running whole training set 

model_params:
  in_channels: 200
  hidden_channels: 128
  out_channels: 128
  norm: "batch_norm"
  pooling_function: "global_max_pool" 
  # pooling function chosen from: global_mean_pool, global_add_pool, global_sort_pool, global_max_pool 
  loss_weights: [0.5, 0.5]
  label_smoothing: 0.01
  #DirGNNConv specific paras: 
  alpha: 0.5
  # GCN model specific: 
  undirectional_graph: False
  # GATv2model specific and TransformConv model
  heads: 4
  concate: True

node_features: 
  identity: True
  correlation_matrix: False
  masking: False
  mask_prob: 0.01

predictor_paras: 
  norm_enabled: False # If add a normalization layer before predictor and before adding to metadata 
  norm_type: "batchnorm"  # or "layernorm"
  norm_timing: "before_meta" # or after 
  # If norm_timing is "before", then a normalization layer is appied before concating with metadata -> normalize only GCN output
  # If norm_time is "after", then a normalization layer is applied after concating with metadata -> normlize everything 
  predictor_type: Linear 
  num_pred_layers: 2 # Only this is hard coded for now 
  activation_btw_predictors: ReLU

### Data
root_folder: "/u/b/i/bingl/private/fMRI-AHDH/data"
# root_folder: "C:\\Users\\tosso\\Documents\\widsdatathon2025"
add_metadata: True
resampling_enabled: True

### Classification type 
task: adhd # adhd, four

### Optimizer


### Hyperparameters
num_folds: 5 # Cross validation
lr: 0.0001
num_layers: 1
dropout: 0.1
batch_size: 16
num_epochs: 250
master_seed: 3407
patience: 20

### scaler: 
scaling:
  enabled: True
  scaler: "MeanStd"

### output
checkpoint_dir: './checkpoints'
wandb:
  enabled: True
  project: ADHD-fMRI